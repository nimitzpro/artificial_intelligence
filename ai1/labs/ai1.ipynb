{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5487fe9e",
   "metadata": {},
   "source": [
    "# Artificial Intelligence I\n",
    "## Assignment 1 - Alexander Stradnic - 119377263"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b379399",
   "metadata": {},
   "source": [
    "## Importing packages, loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b2145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Fundamentals\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "from pandas.plotting import scatter_matrix\n",
    "from seaborn import scatterplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data splitting\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Models to be used\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Hyperparameter selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Scalers, imputers, and transformers\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# One hot encodng\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Error calculation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250e8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/dataset_salaries.csv\")\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=2)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57604d6d",
   "metadata": {},
   "source": [
    "## Filtering the dataset\n",
    "\n",
    "First I make sure that all the salaries are all scaled correctly relative to each other (x | x * 1000 = salary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[df['basesalary'], df['totalyearlycompensation'], df['stockgrantvalue']] = np.where(df['totalyearlycompensation'] > 10000,\n",
    "                                           (df['basesalary'] / 1000, df['totalyearlycompensation'] / 1000, df['stockgrantvalue'] / 1000),\n",
    "                                           (df['basesalary'], df['totalyearlycompensation'], df['stockgrantvalue']))\n",
    "\n",
    "df.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter_matrix(df, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a7d1e4",
   "metadata": {},
   "source": [
    "## Choosing features\n",
    "\n",
    "I decide to select the following numeric and nominal features that I think correlate with the expected salary, including :\n",
    "\n",
    "Numeric:\n",
    "- years of experience\n",
    "- years spent working at the company\n",
    "\n",
    "Categorical:\n",
    "- location <- standard location string\n",
    "    - cityid <- ID number of city\n",
    "    - dmaid <- Designated Market Area, which splits the US into areas by TV broadcaster area\n",
    "- company\n",
    "- tag (area of expertise)\n",
    "- title (role of employee)\n",
    "- level of employee (different labelling per company but mostly similar)\n",
    "\n",
    "I try to avoid unfair human biases by removing gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a2528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "numeric_features = ['yearsofexperience', 'yearsatcompany']\n",
    "nominal_features = ['title', 'location', 'level', 'tag', 'dmaid', 'cityid', 'company']\n",
    "# nominal_features = ['location', 'tag', 'company']\n",
    "features = numeric_features + nominal_features\n",
    "\n",
    "# target_features = ['basesalary', 'bonus', 'stockgrantvalue']\n",
    "target_features = ['basesalary']\n",
    "\n",
    "cols = features + target_features\n",
    "\n",
    "# Removing entries with no salary\n",
    "df = (df[df['basesalary'] > 0]).copy()\n",
    "\n",
    "df2 = pd.DataFrame(df, columns = cols) \n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c4d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off the test set: 20% of the dataset.\n",
    "dev_df, test_df = train_test_split(df2, train_size=0.8, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71800941",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = scatter_matrix(dev_df, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ec1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = scatterplot(x=\"basesalary\", y=\"yearsofexperience\", data=dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = scatterplot(x=\"yearsatcompany\", y=\"yearsofexperience\", data=dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd83abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[numeric_features + target_features].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1c6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb4bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features but leave as a DataFrame\n",
    "dev_X = dev_df[features]\n",
    "test_X = test_df[features]\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "dev_y = dev_df[\"basesalary\"].values\n",
    "test_y = test_df[\"basesalary\"].values\n",
    "\n",
    "# dev_y = pd.DataFrame(dev_df,\n",
    "#                          columns = target_features)\n",
    "# test_y = pd.DataFrame(test_df,\n",
    "#                          columns = target_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b27694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining shuffle split (75% of development data, 60% of total data)\n",
    "ss = ShuffleSplit(n_splits=1, train_size=0.75, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056fd12",
   "metadata": {},
   "source": [
    "I took the MetaTransformer class from the lecture slides as it allows me to choose the most accurate scaler in a given scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104c185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, transformer=None):\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.transformer:\n",
    "            self.transformer.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.transformer:\n",
    "            return self.transformer.transform(X)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b9a1a",
   "metadata": {},
   "source": [
    "Looking at the graph for years of experience compared to the years at the company, it seems that general experience scales at a factor faster than company experience, so I created a function to try balance this relationship and pass it into the regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsertCompExpRatio(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, insert=True):\n",
    "        self.insert = insert\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.insert:\n",
    "            X[\"cer\"] = (X[\"yearsatcompany\"] * 3) / X[\"yearsofexperience\"]\n",
    "    \n",
    "            X = X.replace( [ np.inf, -np.inf ], 0 )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11dfa7",
   "metadata": {},
   "source": [
    "I experimented with feature engineering the Log of years of experience as the yoe/basesalary seems to show a slow initial growth as yoe increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5bce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsertLogExp(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, insert=True):\n",
    "        self.insert = insert\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.insert:\n",
    "            X = X.replace( [ np.inf, -np.inf, np.nan ], 0 )\n",
    "            z = (X[X[\"yearsofexperience\"]>0]).copy()\n",
    "            # X[\"logexp\"] = np.where(X[\"yearsofexperience\"]>0, np.log(X[\"yearsofexperience\"]), X[\"yearsofexperience\"])\n",
    "\n",
    "            X[\"logexp\"] = np.log(z[\"yearsofexperience\"])\n",
    "    \n",
    "            X = X.replace( [ np.inf, -np.inf, np.nan ], 0 )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13749f4c",
   "metadata": {},
   "source": [
    "## Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fef597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"num\", Pipeline([(\"cer\", InsertCompExpRatio()),\n",
    "                        (\"logexp\", InsertLogExp()),\n",
    "                        (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "                        (\"scaler\", MetaTransformer())]), \n",
    "                numeric_features),\n",
    "        (\"nom\", Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")), \n",
    "                        (\"binarizer\", OneHotEncoder(handle_unknown=\"ignore\"))]), \n",
    "                nominal_features)],\n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa7024",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2901c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN Grid Search (using holdout)\n",
    "\n",
    "# Create a pipeline that combines the preprocessor with kNN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor())])\n",
    "\n",
    "# Create a dictionary of hyperparameters and values to try\n",
    "param_grid = {\"predictor__n_neighbors\" : [i for i in range(10,21)],\n",
    "                    \"preprocessor__num__scaler__transformer\": [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "                    \"preprocessor__num__cer__insert\": [True, False],\n",
    "                    \"preprocessor__num__logexp__insert\": [True, False]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "knn_gs = GridSearchCV(knn_model, param_grid, scoring=\"neg_mean_absolute_error\", cv=ss)\n",
    "\n",
    "# Run grid search by calling fit\n",
    "knn_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "knn_gs.best_params_, knn_gs.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN error estimation (n_neighbors=15, cer=False, MinMaxScaler()) on the dev set\n",
    "\n",
    "knn_model.set_params(**knn_gs.best_params_) \n",
    "\n",
    "cross_val_score(knn_model, dev_X, dev_y, scoring=\"neg_mean_absolute_error\", cv=ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28da1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-NEAREST NEIGHBOURS ON THE TEST SET\n",
    "knn_model.fit(dev_X, dev_y)\n",
    "mean_absolute_error(test_y, knn_model.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874bef6",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with linear regression\n",
    "ols = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", LinearRegression())])\n",
    "\n",
    "# Create a dictionary of hyperparameters\n",
    "ols_param_grid = {\"preprocessor__num__scaler__transformer\": [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "                    \"preprocessor__num__cer__insert\": [True, False],\n",
    "                    \"preprocessor__num__logexp__insert\": [True, False]}\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "ols_gs = GridSearchCV(ols, ols_param_grid, scoring=\"neg_mean_absolute_error\", cv=ss, refit=True)\n",
    "\n",
    "# Run grid search by calling fit. . It will also re-train on train+validation using the best parameters.\n",
    "ols_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "ols_gs.best_params_, ols_gs.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ff67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression on the dev set (cer=True, logexp=True, RobustScaler())\n",
    "ols.set_params(**ols_gs.best_params_) \n",
    "ols.fit(dev_X, dev_y)\n",
    "mean_absolute_error(dev_y, ols.predict(dev_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f329beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION ON THE TEST SET\n",
    "mean_absolute_error(test_y, ols.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ded095",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [i for i in range(21)]\n",
    "# Create a pipeline that combines the preprocessor with ridge regression\n",
    "ridge = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", Ridge())])\n",
    "\n",
    "# Create a dictionary of hyperparameters for rideg regression\n",
    "ridge_param_grid = {\"preprocessor__num__scaler__transformer\": [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "                    \"predictor__alpha\": alphas,\n",
    "                    \"preprocessor__num__cer__insert\": [True, False],\n",
    "                    \"preprocessor__num__logexp__insert\": [True, False]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "ridge_gs = GridSearchCV(ridge, ridge_param_grid, scoring=\"neg_mean_absolute_error\", cv=10, refit=True)\n",
    "\n",
    "# Run grid search by calling fit. It will also re-train on train+validation using the best parameters.\n",
    "ridge_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "ridge_gs.best_params_, ridge_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge Regression on the dev set (alpha(lambda)=9, cer=False, logexp=True, MinMaxScaler())\n",
    "precomputed = True\n",
    "if precomputed:\n",
    "    ridge_gs.best_params_ = {'predictor__alpha': 9,\n",
    "  'preprocessor__num__cer__insert': False,\n",
    "  'preprocessor__num__logexp__insert': True,\n",
    "  'preprocessor__num__scaler__transformer': MinMaxScaler()}\n",
    "else:\n",
    "    ridge.set_params(**ridge_gs.best_params_)\n",
    "\n",
    "ridge.fit(dev_X, dev_y)\n",
    "mean_absolute_error(dev_y, ridge.predict(dev_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RIDGE REGRESSION ON THE TEST SET\n",
    "mean_absolute_error(test_y, ridge.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7058a80d",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e664d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", SGDRegressor(max_iter=1000, tol=1e-3, learning_rate=\"optimal\"))])\n",
    "sgd.fit(dev_X, dev_y)\n",
    "\n",
    "# Create a dictionary of hyperparameters\n",
    "sgd_param_grid = {\"preprocessor__num__scaler__transformer\": [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "                    \"preprocessor__num__cer__insert\": [True, False],\n",
    "                    \"preprocessor__num__logexp__insert\": [True, False],\n",
    "                    \"predictor__alpha\": [10**i for i in range(-5, 1)]}\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "sgd_gs = GridSearchCV(sgd, sgd_param_grid, scoring=\"neg_mean_absolute_error\", cv=ss, refit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c4e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search by calling fit. . It will also re-train on train+validation using the best parameters.\n",
    "sgd_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "sgd_gs.best_params_, sgd_gs.best_score_\n",
    "# Linear Regression (gradient descent) on the dev set (cer=True, logexp=True, RobustScaler())\n",
    "sgd.set_params(**sgd_gs.best_params_) \n",
    "sgd.fit(dev_X, dev_y)\n",
    "mean_absolute_error(dev_y, sgd.predict(dev_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b9b0fc",
   "metadata": {},
   "source": [
    "(more tweaking is needed here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eba131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT DESCENT LINEAR REGRESSION ON THE TEST SET\n",
    "mean_absolute_error(test_y, sgd.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1be311",
   "metadata": {},
   "source": [
    "## Experimentation with Polynomial Regression\n",
    "\n",
    "I tried some polynomial functions to see if I could approach or exceed the accuracy of standard linear regression, without excessive computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e08b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_nominals = [\"company\", \"tag\", \"title\"]\n",
    "exp_features = exp_nominals + numeric_features\n",
    "cols = exp_features + [\"basesalary\"]\n",
    "exp_df = pd.DataFrame(df2, columns = cols)\n",
    "\n",
    "# Split off the test set: 20% of the dataset.\n",
    "exp_dev_df, exp_test_df = train_test_split(exp_df, train_size=0.8, random_state=2)\n",
    "\n",
    "# Extract the features but leave as a DataFrame\n",
    "exp_dev_X = exp_dev_df[exp_features]\n",
    "exp_test_X = exp_test_df[exp_features]\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "exp_dev_y = exp_dev_df[\"basesalary\"].values\n",
    "exp_test_y = exp_test_df[\"basesalary\"].values\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"nom\", Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")), \n",
    "                        (\"binarizer\", OneHotEncoder(handle_unknown=\"ignore\", sparse=True))]), \n",
    "                exp_nominals)],\n",
    "        remainder=\"passthrough\")\n",
    "\n",
    "quadratic_model = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"predictor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# np.mean(cross_val_score(quadratic_model, exp_dev_X, exp_dev_y, scoring=\"neg_mean_absolute_error\", cv=10, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc9593",
   "metadata": {},
   "source": [
    "-> -52.258330920038404\n",
    "\n",
    "The code took half an hour to run on 16 threads at full speed and yet produced an output worse than the linear regressors/kNN. Clearly the data is not quadratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b892910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_model.fit(exp_dev_X, exp_dev_y)\n",
    "\n",
    "y_predicted = quadratic_model.predict(exp_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955d3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sct_plt():\n",
    "    fig = plt.figure()\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"y\")\n",
    "    # plt.ylim(-4, 14)\n",
    "    plt.scatter(exp_dev_X[\"yearsofexperience\"], exp_dev_y, color = \"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "sct_plt()\n",
    "plt.plot(exp_test_X[\"yearsofexperience\"], y_predicted, color = \"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c720594",
   "metadata": {},
   "source": [
    "As visible from the chart, the quadratic model significantly overfits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11746b2f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Ridge regression has produced the most accurate results by a small margin followed by Linear Regression and kNN. However, it is clear that the cleanliness of the set plays a large part, and a balance between clean data and amount of data has to be found."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b776c36c8f4fc7ee0637774abc508c94fd4618849f32b4ab8aac89bd966329b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
