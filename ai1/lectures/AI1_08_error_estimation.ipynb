{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Error Estimation</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Our Linear Model</h1>\n",
    "<ul>\n",
    "    <li>We'll repeat the code from the end of a previous lecture.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features we want to select\n",
    "features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract those features and convert to a numpy array\n",
    "X = df[features].values\n",
    "\n",
    "# Extract the target values and convert to a numpy array\n",
    "y = df[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "linear_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>How Good Is This Model?</h1>\n",
    "<ul>\n",
    "    <li>We've built an estimator by learning a model from a dataset.</li>\n",
    "    <li>We want to know how well it will do in practice, once we start to use it to make predictions.\n",
    "        <ul>\n",
    "            <li>This is called <b>error estimation</b>.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Easy right? \n",
    "        <ul>\n",
    "            <li>The dataset comes with <em>actual</em> target values.</li>\n",
    "            <li>We can ask the estimator to <em>predict</em> target values for each example in the dataset.</li>\n",
    "            <li>So now we have actual and predicted values, we can compute the mean squared error.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = linear_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7648.571585887393"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y, y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>But, for at least two reasons, we don't do this!\n",
    "        <ul>\n",
    "            <li>We might want to use a different performance measure than what we used as the loss function.</li>\n",
    "            <li>We want to know how well the model <b>generalizes</b> to <b>unseen data</b>.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Choosing a Different Performance Measure</h1>\n",
    "<ul>\n",
    "    <li>Often in machine learning, we use one measure during learning and another for evaluation.</li>\n",
    "    <li>Our loss function (mean squared error or half of it!) was ideal for learning (why?)\n",
    "        but may not be so good as a performance measure.\n",
    "        <ul>\n",
    "            <li>We could use <b>root mean squared error</b> (RMSE):\n",
    "                $$\\sqrt{\\frac{1}{m}\\sum_{i=1}^m(h_{\\v{\\beta}}(\\v{x}^{(i)}) - \\v{y}^{(i)})^2}$$\n",
    "                i.e don't halve the MSE, and take its square root. RMSE is the standard deviation\n",
    "                of the errors in the predictions.\n",
    "            </li>\n",
    "            <li>Or we could use <b>mean absolute error</b> (MAE):\n",
    "                $$\\frac{1}{m}\\sum_{i=1}^m\\abs(h_{\\v{\\beta}}(\\v{x}^{i)}) - \\v{y}^{(i)})$$\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.62431715362864"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y, y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Generalizing to Unseen Data</h1>\n",
    "<ul>\n",
    "    <li>The error on the <b>training set</b> is called the <b>training error</b>\n",
    "        (also 'resubstitution error' and 'in-sample error').\n",
    "    </li>\n",
    "    <li>But we want to know how well we will perform in the future, on <em>unseen data</em>.\n",
    "        <ul>\n",
    "            <li>The training error is not, in general a good indicator of performance on unseen data.</li>\n",
    "            <li>It's often too optimistic. Why?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>To predict future performance, we need to measure error on an <em>independent</em> dataset:\n",
    "        <ul>\n",
    "            <li>We want a dataset that has played no part in creating the model.</li>\n",
    "            <li>This second dataset is called the <b>test set</b>.</li>\n",
    "            <li>The error on the test set is called the <b>test error</b> (also 'out-of-sample error' and\n",
    "                'extra-sample error').\n",
    "            </li>\n",
    "       </ul>\n",
    "   </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Holdout</h1>\n",
    "<ul>\n",
    "    <li>So we use the following method:\n",
    "       <ul>\n",
    "           <li><em>Shuffle</em> the dataset and <em>partition</em> it into two:\n",
    "               <ul>\n",
    "                   <li>training set (e.g. 80% of the full dataset); and</li>\n",
    "                   <li>test set (the rest of the full dataset).</li>\n",
    "                </ul>\n",
    "           </li>\n",
    "           <li>Train the estimator on the training set.</li>\n",
    "           <li>Test the model (evaluate the predictions) on the test set.</li>\n",
    "       </ul>\n",
    "   </li>\n",
    "   <li>\n",
    "       This method is called the <b>holdout</b> method, because the test set\n",
    "       is withheld (held-out) during training.\n",
    "       <ul>\n",
    "           <li>It is essential that the test set is not used in any way to create\n",
    "               the model.\n",
    "           </li>\n",
    "           <li><em>Don't even look at it!</em>\n",
    "           </li>\n",
    "           <li>'Cheating' is called <b>leakage</b>.</li>\n",
    "           <li>('Cheating' is one cause of <b>overfitting</b>; see later.)</li> <!-- (see <i>CS4619</i>)</li>-->\n",
    "       </ul>\n",
    "    </li>\n",
    "    <li>Class exercise: Standardization, as we know, is about scaling the data. It requires calculation\n",
    "        of the mean and standard deviation. When should the mean and standard deviation be calculated:\n",
    "        (a) before splitting, on the entire dataset, or (b) after splitting, on just the training set?\n",
    "        Why?\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Holdout in scikit-learn: one method</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shufle and split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on the training data\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.44261488699179"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on the test data\n",
    "y_test_predictions = linear_model.predict(X_test)\n",
    "\n",
    "# Calculate MAE on the test data\n",
    "mean_absolute_error(y_test, y_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Holdout in scikit-learn: another way</h2>\n",
    "<ul>\n",
    "    <li>This alternative involves writing less code.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the object that shuffles and splits the data\n",
    "ss = ShuffleSplit(n_splits=1, train_size=0.8, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-64.44261489])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle & split the data, fit the model on the training data, predict on the test data, calculate MAE on the test data\n",
    "cross_val_score(linear_model, X, y, scoring=\"neg_mean_absolute_error\", cv=ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>This is the negative of the MAE &mdash; so that higher values (closer to zero ) are better.</li>\n",
    "    <li>Often this number (the test error) will be higher than the training error.</li>\n",
    "    <li>Run it again with a different random state.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Pros and Cons of Holdout</h1>\n",
    "<ul>\n",
    "    <li>The advantage of holdout is:\n",
    "        <ul>\n",
    "            <li>The test error is independent of the training set.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The disadvantages of this method are:\n",
    "        <ul>\n",
    "            <li>Results can vary quite a lot across different runs.\n",
    "                <ul>\n",
    "                    <li>Informally, you might get lucky &mdash; or unlucky</li>\n",
    "                </ul>\n",
    "                I.e. in any one split, the data used for training or testing might not be representative.\n",
    "            </li>\n",
    "            <li>We are training on only a subset of the available dataset, perhaps as little as 50% of it.\n",
    "                <ul>\n",
    "                    <li>From so little data, we may learn a worse model and so our error measurement may \n",
    "                        be pessimistic.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In practice, we only use the holdout method when we have a very large dataset.\n",
    "        <ul>\n",
    "            <li>The size  of the dataset mitigates the above problems.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        When we have a smaller dataset, we use a <b>resampling</b> method:\n",
    "        <ul>\n",
    "            <li>The examples get re-used for training and testing.</li>\n",
    "        </ul>\n",
    "    </li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>$k$-Fold Cross-Validation</h1>\n",
    "<ul>\n",
    "    <li>The most-used resampling method is $k$-fold cross-validation:\n",
    "        <ul>\n",
    "            <li>Shuffle the dataset and partition it into $k$ disjoint subsets of equal size.\n",
    "                <ul>\n",
    "                    <li>Each of the partitions is called a <b>fold</b>.</li>\n",
    "                    <li>Typically, $k = 10$, so you have 10 folds.</li>\n",
    "                    <!--\n",
    "                    <li>But, for conventional statistical significance testing to be applicable, you should probably ensure \n",
    "                        that the number of examples in each fold does not fall below 30. (If this isn't possible, then either \n",
    "                        use a smaller value for $k$, or do not use $k$-fold cross validation!)\n",
    "                    </li>\n",
    "                    -->\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>You take each fold in turn and use it as the test set, training the learner on \n",
    "                the remaining folds.\n",
    "            </li>\n",
    "            <li>Clearly, you can do this $k$ times, so that each fold gets 'a turn' at being the test set.\n",
    "                <ul>\n",
    "                    <li>\n",
    "                        By this method, each example is used exactly once for testing, and $k - 1$ times for training.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    <li>In pseudocode:\n",
    "        <ul style=\"background: lightgray; list-style: none\">\n",
    "            <li>\n",
    "                shuffle the dataset $D$ and partition it into $k$ disjoint equal-sized subsets, $T_1, T_2,\\ldots,T_k$\n",
    "            <li>\n",
    "            <li>\n",
    "                <b>for</b> $i = 1$ to $k$\n",
    "                <ul>\n",
    "                    <li>train on $D \\setminus T_i$</li>\n",
    "                    <li>make predictions for $T_i$</li>\n",
    "                    <li>measure error (e.g. MAE)</li>\n",
    "                </ul>\n",
    "                report the mean of the errors\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>$k$-Fold Cross Validation in scikit-learn</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-60.79942430798836"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the object that shuffles & splits the data\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=2)\n",
    "\n",
    "# Shuffle & split the data\n",
    "# Repeat k times: fit the model on all but one fold, predict on the remaining fold, calculate MAE\n",
    "# This gives k MAEs, so take their mean\n",
    "np.mean(cross_val_score(linear_model, X, y, scoring=\"neg_mean_absolute_error\", cv=kf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>But $k$-fold cross-validation is so common, there's a shorthand:</li>\n",
    "</ul>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-60.20143514563968"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(linear_model, X, y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Be warned, however, the shorthand does not shuffle the dataset before splitting it into folds.\n",
    "        <ul>\n",
    "            <li>Why might that be a problem?</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If you use the shorthand, you should probably shuffle the <code>DataFrame</code> just after reading it in from \n",
    "        the CSV file (see example below).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pros and Cons of $k$-Fold Cross-Validation</h2>\n",
    "<ul>\n",
    "    <li>Pros:\n",
    "        <ul>\n",
    "            <li>\n",
    "                The test errors of the folds are independent &mdash; because examples are included in only one test set.\n",
    "            </li>\n",
    "            <li>\n",
    "                Better use is made of the dataset: for $k = 10$, for example, we train using 9/10 of the dataset.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Cons:\n",
    "        <ul>\n",
    "            <li>\n",
    "                While the test sets are independent of each other, the training sets are not: \n",
    "                <ul>\n",
    "                    <li>They will overlap with each other to some degree.</li>\n",
    "                    <li>(This effect of this will be less, of course, for larger datasets.)</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>\n",
    "                The number of folds is constrained by the size of the dataset and the desire sometimes on the part of\n",
    "                statisticians to have folds of at least 30 examples.\n",
    "            </li>\n",
    "            <li>\n",
    "                It can be costly to train the learning algorithm $k$ times.\n",
    "            </li>\n",
    "            <li>\n",
    "                There may still be some variability in the results due to 'lucky'/'unlucky' splits.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A Little Case Study</h1>\n",
    "<ul>\n",
    "    <li>Let's learn a linear model and compare it with 3NN.</li>\n",
    "    <li>For 3NN, we will need to standardize the data.</li>\n",
    "    <li>We will also standardize it for the linear model. scikit-learn's <code>LinearRegression</code> class\n",
    "        does not require us to do this but no harm is done by doing it. The advantage is that it makes our\n",
    "        code for the two regressors more consistent.\n",
    "    </li>\n",
    "    <li>We'll use 10-fold cross-validation and we'll use the shorthand so we'll shuffle the dataset ourselves.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkA.csv\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=2)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# The features we want to select\n",
    "features = [\"flarea\", \"bdrms\", \"bthrms\"]\n",
    "\n",
    "# Extract the features but leave as a DataFrame\n",
    "X = df[features]\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "y = df[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"scaler\", StandardScaler(), features)], \n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with the linear model\n",
    "linear_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with 3NN\n",
    "knn_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor(n_neighbors=3))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-60.79942430798835"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error estimation for the linear model\n",
    "np.mean(cross_val_score(linear_model, X, y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-72.60975948196116"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Error estimation for 3NN\n",
    "np.mean(cross_val_score(knn_model, X, y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <ul>\n",
    "    <li>Notice how much work <code>cross_val_score</code> is doing for us:\n",
    "        <ul>\n",
    "            <li>It partitions the data.</li>\n",
    "            <li>Then, on the training set, for each transformer in the pipeline, it calls <code>fit</code>\n",
    "                and <code>transform</code> and, for the predictor at the end of the pipeline, it also\n",
    "                calls <code>fit</code>.\n",
    "                <img src=\"images/pipeline1.png\" />\n",
    "            </li>\n",
    "            <li>Then, on the test set, for each transformer in the pipeline, it calls <code>transform</code> and, \n",
    "                for the predictor at the end of the pipeline, it calls <code>predict</code>.\n",
    "                <img src=\"images/pipeline2.png\" />\n",
    "            </li>\n",
    "            <li>And, in the case of $k$-fold cross-validation, it repeats the above $k$ times.\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Some remarks</h2>\n",
    "<ul>\n",
    "    <li>\n",
    "        In the past, students have tried holdout and $k$-fold as if they were in\n",
    "        competition with each other. This betrays a misunderstanding. You do not try them\n",
    "        both and see which one gives the lower error. You pick one of them &mdash; the one\n",
    "        that makes most sense for your data &mdash; and use it.\n",
    "    </li>\n",
    "    <li>\n",
    "        There are many resampling methods other than $k$-Fold Cross-Validation:\n",
    "        <ul>\n",
    "            <li>Repeated $k$-Fold Cross-Validation, Leave-One-Out-Cross-Validation, \n",
    "                &hellip;\n",
    "            </li>\n",
    "            <li>See the classes in <code>sklearn.model_selection</code>.</li>\n",
    "        </ul>\n",
    "    </li> \n",
    "    <li>\n",
    "        So you've used one of the above methods and found the test error of your predictor.\n",
    "        <ul>\n",
    "            <li>This is supposed to give you an idea of how your predictor will perform in practice.</li>\n",
    "            <li>What if you are dissatisfied with the test error? It seems too high.\n",
    "                <ul>\n",
    "                    <li>It is tempting to tweak your learning algorithm or try different algorithms\n",
    "                        to try to bring down the test error.\n",
    "                    </li>\n",
    "                    <li>This is wrong! It is <b>leakage</b> again: you will be using knowledge of the test \n",
    "                        set to develop the predictor and is likely to result in an optimistic view of the \n",
    "                        ultimate performance of the predictor on unseen data.\n",
    "                    </li>\n",
    "                    <li>Ideally, error estimation on the test set is the last thing you do.\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        Finally, suppose you have used one of the above methods to estimate the error of your predictor. \n",
    "        You are ready to release your predictor on the world. At this point, you can train it on\n",
    "        <em>all</em> the examples in your dataset, so as to maximize the use of the data.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Finishing the case study</h2>\n",
    "<ul>\n",
    "    <li>Since the linear model is better than 3NN, this is the model we will deploy.</li>\n",
    "    <li>At this point, we can retrain on the entire dataset.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('scaler', StandardScaler(),\n",
       "                                                  ['flarea', 'bdrms',\n",
       "                                                   'bthrms'])])),\n",
       "                ('predictor', LinearRegression())])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the linear model on the entire dataset\n",
    "linear_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We can save this model using <code>dump</code> from <code>pickle</code> or <code>joblib</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/my_model.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(linear_model, \"models/my_model.pkl\") # For this to work, create a folder called models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We can read it into, e.g., our web app's backend using <code>load</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load(\"models/my_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Then, when we want to make predictions, we can create a <code>DataFrame</code> of objects for which \n",
    "        we want predictions and call <code>model.predict</code>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
