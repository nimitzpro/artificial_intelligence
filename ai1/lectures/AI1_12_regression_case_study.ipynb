{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4618: Artificial Intelligence I</h1>\n",
    "<h1>Regression Case Study</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from seaborn import scatterplot, heatmap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Introductory Remarks</h1>\n",
    "<ul>\n",
    "    <li>In this lecture, we're going to rattle through the steps of a slightly larger case study.</li>\n",
    "    <li>This will help bring out the importance of some steps we have not mentioned yet, including:\n",
    "        <ul>\n",
    "            <li>Dataset acquisition;</li>\n",
    "            <li>Dataset exploration; and</li>\n",
    "            <li>Dataset preprocessing.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If this were a module about data analytics, then each topic would fill several lectures. \n",
    "        <img src=\"images/data_science.png\" />\n",
    "        (Image adapted from one by Andy Sherpenberg)\n",
    "    </li>\n",
    "    <li>\n",
    "        Since this is\n",
    "        a module about AI, our treatment will be much more cursory.\n",
    "    </li>\n",
    " </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Identify the Problem</h1>\n",
    "<ul>\n",
    "    <li>What are the business objectives?</li>\n",
    "    <li>How do we expect to benefit by building and using an AI system for making predictions?\n",
    "        How will it contribute to the business objectives?\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Select Performance Measures</h1>\n",
    "<ul>\n",
    "    <li>Select a performance measure:\n",
    "        <ul>\n",
    "            <li>E.g. for regression, we might select MAE.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But we might also want a 'reference point' to know whether we have succeeded:\n",
    "        <ul>\n",
    "            <li>E.g. maybe we have succeeded if our MAE is 10% lower than the current system's MAE.</li>\n",
    "            <li>E.g. maybe we must not only be better than the current system but statistically significantly\n",
    "                better (according to some test of statistical significance).\n",
    "            </li>\n",
    "            <li>E.g. maybe we have succeeded if MAE is no greater than some threshold.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Sometimes there is more than one performance measure:\n",
    "        <ul>\n",
    "            <li>E.g. maybe in regression we want to separate the errors by measuring overestimates \n",
    "                separately from underestimates.\n",
    "            </li>\n",
    "            <li>E.g. maybe we want to measure training time or memory used during training.</li>\n",
    "            <li>E.g. maybe we want to measure prediction time or memory used when making a prediction.</li>\n",
    "            <li>E.g. maybe we want an incremental learning algorithm that can easily accommodate new training\n",
    "                examples when they arise.\n",
    "            </li>\n",
    "            <li>E.g. maybe we care about learning an interpretable model or being able to explain the system's\n",
    "                predictions.\n",
    "            </li>\n",
    "            <li>And so on.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Dataset Acquisition</h1>\n",
    "<ul>\n",
    "    <li>Where will it come from? Existing databases and files? By scanning paper documents? By scraping web sites?\n",
    "        &hellip;\n",
    "    </li>\n",
    "    <li>What quantities of data are available? How much do you need?</li>\n",
    "    <li>What format is it in? What will you need to do to convert it?\n",
    "        <ul>\n",
    "            <li>E.g. you may need to de-normalize relational databases or flatten other data structures such as trees\n",
    "        and graphs in order to produce the tabular (matrix) format that most of the scikit-learn learning algorithms expect.</li>\n",
    "            <li>E.g. you may need to handle different character encodings.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Are there legal or ethical issues: copyright, authorization, privacy, bias? E.g. will you need to\n",
    "        anonymize? Will you need to de-bias?\n",
    "    </li>\n",
    "    <li>If you need a labeled dataset, are the labels available? Are they reliable? How will you\n",
    "        obtain them if not? (Get an expert? Get several experts? Use crowdsourcing? Use a data labeling company?)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Cleaning</h1>\n",
    "<ul>\n",
    "    <li>Take a quick look at the data: compute some summary statistics, look at a few examples.</li>\n",
    "    <li>Identify any problems.</li>\n",
    "    <li>Clean the data accordingly:\n",
    "        <ul>\n",
    "            <li>E.g. datasets may contain rows that violate your normal data validation criteria: perhaps \n",
    "                a feature value is required but there are rows where it is missing; perhaps there is a\n",
    "                maximum value for a feature and there are rows where this maximum is exceeded;\n",
    "                perhaps a non-numeric\n",
    "                feature has a finite set of allowed values but some rows contain illegal values for this\n",
    "                feature. You must\n",
    "                either fix these rows (if that is possible) or delete them. \n",
    "            </li>\n",
    "            <li>E.g. if we are doing supervised learning, we need target values: they cannot be missing and\n",
    "                they need to be sensible.\n",
    "            </li>\n",
    "            <li>E.g. we may need to identify and handle duplicated examples.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Note that this is one of several steps where it would be useful to be able to discuss with\n",
    "        a domain expert.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Data cleaning for the Cork Property Prices Dataset</h2>\n",
    "<ul>\n",
    "    <li>Let's illustrate data cleaning for a different (less clean) version of the Cork Property Prices Dataset.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"../datasets/dataset_corkB.csv\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=2)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(711, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 711 entries, 0 to 710\n",
      "Data columns (total 9 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   flarea    583 non-null    float64\n",
      " 1   type      711 non-null    object \n",
      " 2   bdrms     707 non-null    float64\n",
      " 3   bthrms    711 non-null    int64  \n",
      " 4   floors    711 non-null    int64  \n",
      " 5   devment   706 non-null    object \n",
      " 6   ber       560 non-null    object \n",
      " 7   location  711 non-null    object \n",
      " 8   price     690 non-null    float64\n",
      "dtypes: float64(3), int64(2), object(4)\n",
      "memory usage: 50.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# The columns, their datatypes and whether there are nulls\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flarea</th>\n",
       "      <th>type</th>\n",
       "      <th>bdrms</th>\n",
       "      <th>bthrms</th>\n",
       "      <th>floors</th>\n",
       "      <th>devment</th>\n",
       "      <th>ber</th>\n",
       "      <th>location</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>583.000000</td>\n",
       "      <td>711</td>\n",
       "      <td>707.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>706</td>\n",
       "      <td>560</td>\n",
       "      <td>711</td>\n",
       "      <td>690.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>detached</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SecondHand</td>\n",
       "      <td>C1</td>\n",
       "      <td>Cork City</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>700</td>\n",
       "      <td>64</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>139.016226</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.287129</td>\n",
       "      <td>2.039381</td>\n",
       "      <td>1.853727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>333.627536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>157.192735</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.146934</td>\n",
       "      <td>1.146272</td>\n",
       "      <td>0.457777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>191.264933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>80.540000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>108.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>142.450000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             flarea      type       bdrms      bthrms      floors     devment  \\\n",
       "count    583.000000       711  707.000000  711.000000  711.000000         706   \n",
       "unique          NaN         4         NaN         NaN         NaN           2   \n",
       "top             NaN  detached         NaN         NaN         NaN  SecondHand   \n",
       "freq            NaN       221         NaN         NaN         NaN         700   \n",
       "mean     139.016226       NaN    3.287129    2.039381    1.853727         NaN   \n",
       "std      157.192735       NaN    1.146934    1.146272    0.457777         NaN   \n",
       "min        8.100000       NaN    1.000000    1.000000    1.000000         NaN   \n",
       "25%       80.540000       NaN    3.000000    1.000000    2.000000         NaN   \n",
       "50%      108.000000       NaN    3.000000    2.000000    2.000000         NaN   \n",
       "75%      142.450000       NaN    4.000000    3.000000    2.000000         NaN   \n",
       "max     2000.000000       NaN   15.000000   15.000000    4.000000         NaN   \n",
       "\n",
       "        ber   location        price  \n",
       "count   560        711   690.000000  \n",
       "unique   14         47          NaN  \n",
       "top      C1  Cork City          NaN  \n",
       "freq     64        130          NaN  \n",
       "mean    NaN        NaN   333.627536  \n",
       "std     NaN        NaN   191.264933  \n",
       "min     NaN        NaN    50.000000  \n",
       "25%     NaN        NaN   225.000000  \n",
       "50%     NaN        NaN   280.000000  \n",
       "75%     NaN        NaN   380.000000  \n",
       "max     NaN        NaN  2000.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe have a look at a few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"flarea\", \"bdrms\", \"bthrms\", \"floors\", \"type\", \"devment\", \"ber\", \"location\"]\n",
    "numeric_features = [\"flarea\", \"bdrms\", \"bthrms\", \"floors\"]\n",
    "nominal_features = [\"type\", \"devment\", \"ber\", \"location\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The values, in the case of nominal-valued features\n",
    "for feature in nominal_features:\n",
    "    print(feature, df[feature].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>What problems do you see?</li>\n",
    "    <li>Let's assume that, according to our normal data validation criteria, it is invalid to supply\n",
    "        no value for $\\mathit{devment}$ but it is OK to supply no value for $\\mathit{ber}$. So we will delete\n",
    "        rows that have NaN for their $\\mathit{devment}$.\n",
    "    </li>\n",
    "    <li>Let's assume similarly that values for $\\mathit{flarea}$ cannot be missing and must be greater\n",
    "        than or equal to 40 and less than 750. So we will delete rows that violate these conditions too.\n",
    "    </li>\n",
    "    <li>Let's assume we also want to delete rows where $\\mathit{bdrms}$ is NaN. \n",
    "    </li>\n",
    "    <li>We must delete the rows where $\\mathit{price}$ is NaN because we are doing supervised learning!\n",
    "        But I fear that some of these prices are wrong too. A scatter plot helps show why.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = scatterplot(x=\"flarea\", y=\"price\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>So let's look at the row(s) that have the very high price tag:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"price\"] >= 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Undoubtedly a typo! So we will delete this row too.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete examples where flarea, devment or price are NaN\n",
    "df.dropna(subset=[\"flarea\", \"bdrms\", \"devment\", \"price\"], inplace=True)\n",
    "\n",
    "# Delete examples whose floor areas are too small or too big\n",
    "df = (df[(df[\"flarea\"] >= 40) & (df[\"flarea\"] < 750)]).copy()\n",
    "\n",
    "# Delete examples whose prices are too high\n",
    "df = (df[df[\"price\"] < 2000]).copy()\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Check the invalid data was filtered out\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create a Test Set</h1>\n",
    "<ul>\n",
    "    <li>Split off a test set.</li>\n",
    "    <li>Don't look at it: everything we do until our final error estimation will be done on the rest of\n",
    "        the dataset.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split off the test set: 20% of the dataset.\n",
    "dev_df, test_df = train_test_split(df, train_size=0.8, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset Exploration</h1>\n",
    "<ul>\n",
    "    <li>Time spent just exploring the data is always worthwhile &mdash; especially if you have \n",
    "        access to a domain expert while you are doing so.\n",
    "    </li>\n",
    "    <li><b>Visualization</b> is a great help at this stage.\n",
    "        <ul>\n",
    "            <li>Given a labeled dataset, we often want to see how target values are distributed \n",
    "                with respect to values of one or more of the features.\n",
    "            </li>\n",
    "            <li>Given a labeled or unlabeled dataset, we might want to see how values of one feature are \n",
    "                correlated with values of one or more other features.\n",
    "            </li>\n",
    "        </ul>\n",
    "        A limitation is that visualizations are confined to two (or maybe three) columns of the dataset.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It can be good to do this on a copy of the dataset (excluding the test set, of course)\n",
    "copy_df = dev_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = scatter_matrix(copy_df, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>For any that look interesting, we can draw a bigger scatter plot.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = scatterplot(x=\"flarea\", y=\"bdrms\", data=copy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We can compute correlations between numeric-valued columns.</li>\n",
    "    <li>Most common is Pearson correlation, which measures linear correlation. Its value lies between +1 and −1. \n",
    "        A value of +1 is total positive linear correlation, 0 is no linear correlation, and −1 is total \n",
    "        negative linear correlation.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    If you prefer, you can show this as a heatmap instead, or as well:\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(copy_df.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>We see that $\\mathit{flarea}$ is strongly predictive of $\\mathit{price}$.</li>\n",
    "    <li>We see that $\\mathit{flarea}$, $\\mathit{bdrms}$ and $\\mathit{bthrms}$ are quite strongly correlated\n",
    "        with each other.\n",
    "    </li>\n",
    "    <li>We see that $\\mathit{bdrms}$ and $\\mathit{floors}$ are fairly strongly correlated.</li>\n",
    "    <li>We can add other features, ones that are computed from the existing features &mdash; later, we call this\n",
    "        <b>feature engineering</b>. \n",
    "        <ul>\n",
    "            <li>In general, these new features might be products or ratios of existing features.</li>\n",
    "            <li>Or they might result from applying functions to existing features, e.g. squaring, square rooting,\n",
    "                taking the log, &hellip;\n",
    "            </li>\n",
    "            <li>If you're going to be learning a linear model then it will not be useful to the linear model to add or subtract features or to multiply or divide by a scalar,\n",
    "                since this just gives new features that are linearly correlated with existing features.\n",
    "            </li>\n",
    "        </ul>\n",
    "        We can then produce visualizations and compute correlations to\n",
    "        see whether these new features are predictive or not.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df[\"room_size\"] = copy_df[\"flarea\"] / (copy_df[\"bdrms\"] + copy_df[\"bthrms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df[[\"room_size\", \"flarea\", \"bdrms\", \"bthrms\", \"price\"]].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>E.g. this new feature is quite predictive of price, but highly correlated with floor area.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dataset Preparation</h1>\n",
    "<ul>\n",
    "    <li>We need to prepare the dataset so that it is suitable for machine learning algorithms.</li>\n",
    "    <li>An incomplete lists of activities follows.</li>\n",
    "    <li>Missing values:\n",
    "        <ul>\n",
    "            <li>Most learning algorithms cannot handle missing values.</li>\n",
    "            <li>We may have removed most of them during data cleaning.</li>\n",
    "            <li>If any remain, we need a method to <b>impute</b> a value.\n",
    "                <ul>\n",
    "                    <li>For numeric-valued features, we can replace missing values by the mean, for example.\n",
    "                        (In scikit-learn, use the <code>SimpleImputer</code> with <code>strategy=\"mean\"</code>,\n",
    "                        which is the default.)\n",
    "                    </li>\n",
    "                    <li>For nominal-valued features, we can replace by the mode.\n",
    "                        (<code>strategy=\"most_frequent\"</code>)\n",
    "                    </li>\n",
    "                    <li>There are other possibilities, e.g.: replace by some constant; binarize the feature\n",
    "                        (0 if the value was missing, 1 if not); predict the value from the other features.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Scaling numeric-valued features:\n",
    "        <ul>\n",
    "            <li>Some algorithms (e.g. kNN) perform less well if features have different ranges; others\n",
    "                (e.g. linear regression done using the normal equation) work equally well whether the\n",
    "                data is scaled or not.\n",
    "            <li>We've mentioned two ways to scale: min-max scaling and standardization. There are others.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Feature engineering:\n",
    "        <ul>\n",
    "            <li>In this module we use the phrase <b>feature engineering</b> for the following:\n",
    "                augmenting the dataset with features that are computed from the other features.\n",
    "            </li>\n",
    "            <li>These may have been identified during dataset exploration above.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Feature selection:\n",
    "        <ul>\n",
    "            <li>There are methods for removing features that have low predictive power.\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Dimensionality reduction:\n",
    "        <ul>\n",
    "            <li>An example of this is Principal Component Analysis. Ignoring the details, it transforms\n",
    "                the features into new features that are not linearly correlated with one another\n",
    "                and enables us to discard the new features that contribute least.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Handling nominal-valued features:\n",
    "        <ul>\n",
    "            <li>Non-numeric features deserve separate explanation below.\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Handling Nominal-Valued Features</h2>\n",
    "<ul>\n",
    "    <li>Most AI algorithms work only with numeric-valued features.</li>\n",
    "    <li>So, we will look at how to convert nominal-valued features to numeric-valued ones.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Binary-valued features</h3>\n",
    "<ul>\n",
    "    <li>The simplest case, obviously, is a binary-valued feature, such as $\\mathit{devment}$.</li>\n",
    "    <li>We encode one value as 0 and the other as 1, e.g. \"SecondHand\" is 0 and \"New\" is 1.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Unordered nominal values</h3>\n",
    "<ul>\n",
    "    <li>Suppose there are more than two values, e.g. Apartment, Detached, Semi-detached or Terraced.</li>\n",
    "    <li>The obvious thing to do is to assign integers to each nominal value, e.g. 0 = Apartment, 1 = Detached, \n",
    "        2 = Semi-detached and 3 = Terraced.\n",
    "    </li>\n",
    "    <li>But often this is not the best encoding.\n",
    "        <ul>\n",
    "            <li>Algorithms may assume that the values themselves are meaningful, when they're actually arbitrary.\n",
    "                <ul>\n",
    "                    <li>E.g. an algorithm might assume that Apartments (0) are more similar to Detached houses (1)\n",
    "                        than they are to Terraced houses (3).\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Instead, we use <b>one-hot encoding</b>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>One-Hot Encoding</h3>\n",
    "<ul>\n",
    "    <li>If the original nominal-valued feature has $p$ values, then we use $p$ binary-valued features: \n",
    "        <ul>\n",
    "            <li>In each example, exactly one of them is set to 1 and the rest are zero.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>For example, there are four types of dwelling, so we have four binary-valued features:\n",
    "        <ul>\n",
    "            <li>the first is set to 1 if and only if the type of dwelling is Apartment;</li>\n",
    "            <li>the second is set to 1 if and only if the house is Detached;</li>\n",
    "            <li>and so on.</li>\n",
    "        </ul>\n",
    "        So a detached house will be represented by $\\rv{0, 1, 0, 0}$.\n",
    "    </li>\n",
    "    <li>\n",
    "        In practice, it is not uncommon to be given a dataset where a nominal-valued feature has already been \n",
    "        encoded numerically, one integer per value. You might be fooled into thinking that the feature is\n",
    "        numeric-valued and overlook the need to use one-hot encoding on it. Watch out for this!\n",
    "    </li>\n",
    "    <li>One-hot encoding turns each $p$-valued non-numeric feature into $p$ binary-valued features. Thus, you\n",
    "        can end up with a lot of features. Maybe you need to do some dimensionality reduction afterwards.\n",
    "    </li>\n",
    "    <li>Advanced (ignore): If you think about it, you'll realise you only need $p - 1$ features, not $p$. Why? Using just $p - 1$ may have an advantage, especially if your model uses matrix inversion (e.g. linear regression using the Normal Equation): it reduces colinearities between the features. If you want to try it, then include <code>drop=\"first\"</code> as a parameter to the <code>OneHotEncoder</code> class. But, on the other hand, using just $p - 1$ features has a problem: it conflicts with <code>handle_unknown=\"ignore\"</code>. In what way?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data preprocessing in scikit-learn</h2>\n",
    "<ul>\n",
    "    <li>Clearly, we want to automate all these data preparation activities.</li>\n",
    "    <li>In scikit-learn, it is best to bring all of them together using a <code>ColumnTransformer</code>,\n",
    "        <code>Pipelines</code> and, sometimes, <code>FeatureUnions</code>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "        (\"scaler\", StandardScaler(), \n",
    "                numeric_features),\n",
    "        (\"nom\", Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")), \n",
    "                          (\"binarizer\", OneHotEncoder(handle_unknown=\"ignore\"))]), \n",
    "                nominal_features)],\n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Let's say we want to do some feature engineering. We could add extra features in the way we did\n",
    "        when we were exploring the dataset, e.g.\n",
    "        <pre>\n",
    "        df[\"room_size\"] = df[\"flarea\"] / (df[\"bdrms\"] + df[\"bthrms\"])\n",
    "        </pre>\n",
    "        The problem is that this code is separate from our <code>ColumnTransformer</code>.\n",
    "        And this leads to complications: for example, if we save our model, then this part of it won't be saved.\n",
    "        If we can make it part of the <code>ColumnTransformer</code>, then it wll be saved along\n",
    "        with everything else.\n",
    "    </li>\n",
    "    <li>The solution is to write our own transformer.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsertRoomSize(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, insert=True):\n",
    "        self.insert = insert\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.insert:\n",
    "            X[\"room_size\"] = X[\"flarea\"] / (X[\"bdrms\"] + X[\"bthrms\"])\n",
    "            \n",
    "            # If the new feature is intended to replace the existing ones, \n",
    "            # you could drop the existing ones here\n",
    "            # X.drop([\"flarea\", \"bthrms\", \"bdrms\"], axis=1)\n",
    "    \n",
    "            X = X.replace( [ np.inf, -np.inf ], np.nan )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Now our preprocessor might look like this:</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        (\"num\", Pipeline([(\"room_size\", InsertRoomSize()),\n",
    "                          (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "                          (\"scaler\", StandardScaler())]), \n",
    "                numeric_features),\n",
    "        (\"nom\", Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")), \n",
    "                          (\"binarizer\", OneHotEncoder(handle_unknown=\"ignore\"))]), \n",
    "                nominal_features)],\n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Prepare for Model Selection</h1>\n",
    "<ul>\n",
    "    <li>We have already split off our test set.</li>\n",
    "    <li>But now we decide how to do model selection (see previous lecture):\n",
    "        <ul>\n",
    "            <li>Should we just split the remaining data into a training set and validation set?</li>\n",
    "            <li>Or should we use $k$-fold cross-validation?</li>\n",
    "        </ul>\n",
    "        As discussed, it depends on how much data you have.\n",
    "        <ul>\n",
    "            <li>Since our dataset is quite small, we'll use $k$-fold cross-validation.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We must also decide which models we will experiment with.\n",
    "        <ul>\n",
    "            <li>We'll use linear regression and kNN.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>And for each model, we must set up a grid of hyperparameter values.\n",
    "        <ul>\n",
    "            <li>For kNN, we need to try some different values for $k$.</li>\n",
    "            <li>But we did something clever earlier when creating our own feature engineering transformer.\n",
    "                We included an argument called <code>insert</code>. If this is True, then\n",
    "                we add the extra feature; if it is False, we do not. Hence, this is a hyperparameter.\n",
    "                We can try both values to see whether adding the feature helps or not.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features but leave as a DataFrame\n",
    "dev_X = dev_df[features]\n",
    "test_X = test_df[features]\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "dev_y = dev_df[\"price\"].values\n",
    "test_y = test_df[\"price\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with kNN\n",
    "knn = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor())])\n",
    "\n",
    "# Create a dictionary of hyperparameters for kNN\n",
    "knn_param_grid = {\"predictor__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],\n",
    "                  \"preprocessor__num__room_size__insert\": [True, False]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "knn_gs = GridSearchCV(knn, knn_param_grid, scoring=\"neg_mean_absolute_error\", cv=10, refit=True)\n",
    "\n",
    "# Run grid search by calling fit. It will also re-train on train+validation using the best parameters.\n",
    "knn_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "knn_gs.best_params_, knn_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with ridge regression\n",
    "ridge = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", Ridge())])\n",
    "\n",
    "# Create a dictionary of hyperparameters for ridge regression\n",
    "ridge_param_grid = {\"preprocessor__num__room_size__insert\": [True, False],\n",
    "                    \"predictor__alpha\": [0, 45.0, 50.0, 55.0]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "ridge_gs = GridSearchCV(ridge, ridge_param_grid, scoring=\"neg_mean_absolute_error\", cv=10, refit=True)\n",
    "\n",
    "# Run grid search by calling fit. It will also re-train on train+validation using the best parameters.\n",
    "ridge_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "ridge_gs.best_params_, ridge_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>In fact, this gives me the idea of writing a transformer which takes in another transformer.\n",
    "        Sounds uselesss. But, now that gives me a way of having transformers as hyperparameters.\n",
    "    </li>\n",
    "    <li>The example below uses this to select between a <code>StandardScaler</code>, a <code>MinMaxScaler</code>\n",
    "        and a <code>RobustScaler</code>.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, transformer=None):\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.transformer:\n",
    "            self.transformer.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.transformer:\n",
    "            return self.transformer.transform(X)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        (\"num\", Pipeline([(\"room_size\", InsertRoomSize()),\n",
    "                          (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "                          (\"scaler\", MetaTransformer())]), \n",
    "                numeric_features),\n",
    "        (\"nom\", Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")), \n",
    "                          (\"binarizer\", OneHotEncoder(handle_unknown=\"ignore\"))]), \n",
    "                nominal_features)],\n",
    "        remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with kNN\n",
    "knn = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor())])\n",
    "\n",
    "# Create a dictionary of hyperparameters for kNN\n",
    "knn_param_grid = {\"predictor__n_neighbors\": [8, 9, 10],\n",
    "                  \"preprocessor__num__room_size__insert\": [True, False],\n",
    "                  \"preprocessor__num__scaler__transformer\": [StandardScaler(), MinMaxScaler(), RobustScaler()]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "knn_gs = GridSearchCV(knn, knn_param_grid, scoring=\"neg_mean_absolute_error\", cv=10, refit=True)\n",
    "\n",
    "# Run grid search by calling fit. It will also re-train on train+validation using the best parameters.\n",
    "knn_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "knn_gs.best_params_, knn_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with ridge regression\n",
    "ridge = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", Ridge())])\n",
    "\n",
    "# Create a dictionary of hyperparameters for rideg regression\n",
    "ridge_param_grid = {\"preprocessor__num__room_size__insert\": [True, False],\n",
    "                     \"preprocessor__num__scaler__transformer\": [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "                     \"predictor__alpha\": [0, 45.0, 50.0, 55.0]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "ridge_gs = GridSearchCV(ridge, ridge_param_grid, scoring=\"neg_mean_absolute_error\", cv=10, refit=True)\n",
    "\n",
    "# Run grid search by calling fit. It will also re-train on train+validation using the best parameters.\n",
    "ridge_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "ridge_gs.best_params_, ridge_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tweak and Fine Tune</h1>\n",
    "<ul>\n",
    "    <li>Now tweak and tune your model(s).\n",
    "        <ul>\n",
    "            <li>E.g. go back and change the grid of hyperparameter values.</li>\n",
    "            <li>E.g. go back and add or remove transformers.</li>\n",
    "            <li>E.g. go back and try regressors other than linear regression and kNN.</li>\n",
    "            <li>E.g. you could even go back to the Dataset Acquisition step: maybe try to collect\n",
    "                more features, for example. \n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>You don't have to do this wholly at random. You can be guided in part by knowing whether you\n",
    "        are underfitting or overfitting.\n",
    "        <ul>\n",
    "            <li>E.g. if you are underfitting, then collecting new features or doing feature engineering may help.\n",
    "            </li>\n",
    "            <li>But if you are overfitting then selecting among your features may be better.\n",
    "            </li>\n",
    "        </ul>\n",
    "        See previous two lectures.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.set_params(**knn_gs.best_params_) \n",
    "scores = cross_validate(knn, dev_X, dev_y, cv=10, \n",
    "                        scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "print(\"Training error: \", np.mean(np.abs(scores[\"train_score\"])))\n",
    "print(\"Validation error: \", np.mean(np.abs(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.set_params(**ridge_gs.best_params_) \n",
    "scores = cross_validate(ridge, dev_X, dev_y, cv=10, \n",
    "                        scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "print(\"Training error: \", np.mean(np.abs(scores[\"train_score\"])))\n",
    "print(\"Validation error: \", np.mean(np.abs(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>You can tweak and tune as much as you like!\n",
    "        Keep doing this until you find a good model.\n",
    "    </li>\n",
    "    <li>Of course, if you're not careful, you'll overdo it and produce a model that overfits the training\n",
    "        data.\n",
    "    </li>\n",
    "    <li>The main thing you shouldn't do, of course, is look at the test set until you have finished all\n",
    "        this tweaking and tuning.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Evaluate on the Test Set</h1>\n",
    "<ul>\n",
    "    <li>Once you've done, then, using the best model(s) we found, we can do error estimation on the test set.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we test on the test set\n",
    "mean_absolute_error(test_y, knn_gs.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Deploy</h1>\n",
    "<ul>\n",
    "    <li>Make a decision: is this predictor good enough for real use?\n",
    "        <ul>\n",
    "            <li>You may be comparing one model against other models (e.g. linear regression against kNN regression) or against baselines (e.g. predicting the mean) or against current practice (e.g. your current manual or non-AI system).\n",
    "            </li>\n",
    "            <li>To make this comparison more robust, you might compute confidence intervals (see, e.g., \n",
    "                <a href=\"https://sebastianraschka.com/blog/2022/confidence-intervals-for-ml.html\">this tutorial</a>) or carry out a test for statistical significance. Unfortunately, we don't have time to study these in this module.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If it is, then re-train on the entire dataset, save the model and use it in your web app.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_gs.fit(df[features], df[\"price\"].values)\n",
    "dump(knn, 'models/my_model.pkl') # For this to work, create a folder called models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>Of course, even then your work is not finished:\n",
    "        <ul>\n",
    "            <li>You need to monitor peformance.</li>\n",
    "            <li>You may need to re-train it when fresh data becomes available.</li>\n",
    "            <li>And so on.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
