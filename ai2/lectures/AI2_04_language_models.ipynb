{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Language Models</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import get_file\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Acknowledgement</h1>\n",
    "<ul>\n",
    "     <li>The code comes from: \n",
    "        A. G&eacute;ron: \n",
    "        <i>Hands-On Machine Learning with Scikit-Learn, Keras and TensorFlow (2nd edn)</i>, O'Reilly, 2019\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Warning</h1>\n",
    "<ul>\n",
    "    <li>The code takes a very long time to run.\n",
    "    </li>\n",
    "    <li>It is not important to understand this code in any case.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Natural Language Processing</h1>\n",
    "<ul>\n",
    "    <li>In the previous lectures, we looked at a task (sentiment analysis) that requires <i>natural language\n",
    "        understanding</i>. We tried various approaches including word embeddings and recurrent neural networks.\n",
    "    </li>\n",
    "    <li>In this lecture, we look at  <i>language models</i> and use them for \n",
    "        <i>natural language generation</i> &mdash; producing language.</li>\n",
    "    <li>Specifically, \n",
    "        <ul>\n",
    "            <li>we build a model from training data that can predict the next character in a sentence;</li>\n",
    "            <li>we then use that model, with a bit of randomization, to produce new sentences in the style of\n",
    "                the original training data.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We do this at the level of individual characters, but it can be done at a higher-level, e.g.\n",
    "        predicting/generating the next word.\n",
    "    </li>\n",
    "    <li>This might seem frivolous, but it gives insight into a number of useful systems that we will\n",
    "        cover in the next lecture.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Language Models</h1>\n",
    "<ul>\n",
    "    <li>A <b>language model</b> for a given natural language, such as English, estimates the probability of\n",
    "        each possible string of words, e.g.\n",
    "        <ul>\n",
    "            <li>P(\"The dog chased the cat\") = 0.000002</li>\n",
    "            <li>P(\"The cat chased the dog\") = 0.0000002</li>\n",
    "            <li>P(\"The the chased cat dog\") = 0.000000000000001</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If we have a character-level language model, then we can predict the most-likely next character.\n",
    "        <ul>\n",
    "            <li>E.g. P(\"h\" | \"The dog chased t\") = 0.9, P(\"w\" | \"The dog chased t\") = 0.05, P(\"x\" | \"The dog chased t\") = 0.00001</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>If we have a word-level language model, then we can predict the most-likely next word.\n",
    "        <ul>\n",
    "            <li>E.g. P(\"the\" | \"The dog chased\") = 0.7, P(\"a\" | \"The dog chased\") = 0.689, P(\"walked\" | \"The dog chased\") = 0.0000004</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning a language model</h2>\n",
    "<ul>\n",
    "    <li>If we have lots of text, we can learn a language model.</li>\n",
    "    <li>A simple-minded approach (using a word-level language model by way of example):\n",
    "        <ul>\n",
    "            <li>For each word, count next-word frequncies in the training examples.</li>\n",
    "            <li>E.g. in the training examples, \"the\" is followed by \"dog\" 20 times, by \"cat\" 15 times, \"kangaroo\" once, and so on.\n",
    "            </li>\n",
    "            <li>From these, we can calculate the probabilities.</li>\n",
    "        </ul>\n",
    "        What is the weakness of this?\n",
    "    </li>\n",
    "    <li>So, instead, AI researchers use recurrent neural networks.</li>\n",
    "    <li>We'll illustrate with a character-level language model.\n",
    "        <ul>\n",
    "            <li>Word-level language models have probably been more common.</li>\n",
    "            <li>But, with faster hardware, character-level models are becoming a ltitle more common now.</li>\n",
    "            <li>An advantage of character-level models is we have a small number of next possible characters.\n",
    "                For word-level models, on the other hand, we have to decide on a vocabulary and how to handle words that fall outside the vocabulary.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A Character-Level Language Model using a RNN</h1>\n",
    "<ul>\n",
    "    <li>Everyone does this on Shakespeare &mdash; perhaps because if it outputs bad\n",
    "        Shakespeare some people still think it sounds like Shakespeare!\n",
    "    </li>  \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Preprocessing the training data</h2>\n",
    "<ul>\n",
    "    <li>Most of the effort goes into preprocessing the dataset. Don't get bogged down in the details of this code.</li>\n",
    "    <li>We're one-hot encoding the characters.</li>\n",
    "    <li>We're making overlapping windows, shuffling these, and putting them into batches.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much Shakespeare are we working with? How many characters?\n",
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you the first part of it\n",
    "shakespeare_text[:148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you all its distinct characters\n",
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a character-level (rather than word-level) tokenizer\n",
    "# In effect, it lowercases and assigns ids to characters from 1 to 39 inc, e.g. ' ' is 1, 'e' is 2, etc.\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 23, 2, 5, 25]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you an encoding\n",
    "tokenizer.texts_to_sequences([\"speak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s p e a k']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show you the reverse\n",
    "tokenizer.sequences_to_texts([[8, 23, 2, 5, 25]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the full text (subtract 1 to use ids from 0 to 38 instead of 1 to 39, so now ' ' is 0, 'e' is 1, etc.)\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-26 13:52:42.304866: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-26 13:52:42.334757: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc86fdbb840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-08-26 13:52:42.334771: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "# We'll train on the whole dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In general, we can't train on the whole training set in one go: too long\n",
    "# window() splits this into smaller windows of text\n",
    "# Using shift=1 means the first window is characters 0 to 100, the second is characters 1 to 101, etc.\n",
    "# Using drop_remainder=True means all windows are 101 characters long without needing us to pad the\n",
    "# last ones (they are dropped)\n",
    "# But window() produces a nested dataset: a dataset containing windows (each of which is a dataset)\n",
    "# so we flatten it\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # to include the target\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the windows and put into batches\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the inputs (the first 100 characters) from the targets (the last, i.e. 101st, character)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the moment, characters are replaced by ids. Now we one-hot encode them.\n",
    "# (This is OK for Char-RNN. If we were doing something with words, we might use word embeddings)\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetching means while training on one batch, the next is being prepared\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "# Show you the shape of the first batch (Note: one output per timestep)\n",
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The RNN</h2>\n",
    "<ul>\n",
    "    <li>The input shape is <code>[None, max_id]</code> because of the one-hot encoding.</li>\n",
    "    <li>We'll use a couple of GRU layers with dropout on their inputs and their hidden state.</li>\n",
    "    <li>The output layer has <code>max_id</code> neurons, because we're predicting that number of\n",
    "        distinct characters, i.e. we have <code>max_id</code> classes.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(n_steps, max_id))\n",
    "x = GRU(128, activation=\"tanh\", return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(inputs)\n",
    "x = GRU(128, activation=\"tanh\", return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "outputs = Dense(max_id, activation=\"softmax\")(x)\n",
    "char_language_model = Sequential(Model(inputs, outputs))\n",
    "\n",
    "char_language_model.compile(optimizer=SGD(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a very long time\n",
    "history = char_language_model.fit(dataset, epochs=25, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Self-supervised learning</h2>\n",
    "<ul>\n",
    "    <li>Hold on! We are doing supervised learning. But our dataset has no labels. It is simply a lot of\n",
    "        text.\n",
    "    </li>\n",
    "    <li>So, what are we using as labels?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predictions using the language model</h2>\n",
    "<ul>\n",
    "    <li>Given some text (suitably preprocessed), the model can predict the next character\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to preprocess the text whose next character we will predict: tokenize and one-hot encode\n",
    "def preprocess(text):\n",
    "    X = np.array(tokenizer.texts_to_sequences(text)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 100, 39) for input Tensor(\"functional_1_input:0\", shape=(None, 100, 39), dtype=float32), but it was called on an input with incompatible shape (None, 10, 39).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100, 39) for input Tensor(\"input_1:0\", shape=(None, 100, 39), dtype=float32), but it was called on an input with incompatible shape (None, 10, 39).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = preprocess([\"How are yo\"]) \n",
    "prediction = np.argmax(char_language_model.predict(input_text), axis=-1)\n",
    "tokenizer.sequences_to_texts(prediction + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating text using the language model</h2>\n",
    "<ul>\n",
    "    <li>To generate text, we want to make repeated predictions:\n",
    "        <ul>\n",
    "            <li>Feed in some initial input;</li>\n",
    "            <li>Predict the most likely next character;</li>\n",
    "            <li>Add the prediction to the end of the input text;</li>\n",
    "            <li>Feed in the extended input;</li>\n",
    "            <li>Predict the most likely next character;</li>\n",
    "        </ul>\n",
    "        and so on.\n",
    "    </li>\n",
    "    <li>But this results in output text that is very repetitive.</li>\n",
    "    <li>Instead, we make it stochastic:\n",
    "        <ul>\n",
    "            <li>We pick the next character randomly but based on the probabilities that the network produces.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The temperature parameter allows you to tune it: \n",
    "# - a value close to zero favours high probability characters, but leads to more repetition\n",
    "# - a high value gives all characters an almost equal probability\n",
    "def next_char(model, text, temperature=1):\n",
    "    X = preprocess([text])\n",
    "    y_proba = model.predict(X)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def generate_text(model, text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(model, text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 100, 39) for input Tensor(\"functional_1_input:0\", shape=(None, 100, 39), dtype=float32), but it was called on an input with incompatible shape (None, 1, 39).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100, 39) for input Tensor(\"input_1:0\", shape=(None, 100, 39), dtype=float32), but it was called on an input with incompatible shape (None, 1, 39).\n",
      "Low temperature:\n",
      " t the the sore and the the the has the the sour the \n",
      "\n",
      "Medium temperature:\n",
      " t\n",
      "\n",
      "concons,\n",
      "is kave: fomer fos sitn!\n",
      "a the best oor \n",
      "\n",
      "High temperature:\n",
      " tinl&'ty? by niopimame'l:owa brin hrucg ics spl. bu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some examples\n",
    "print(\"Low temperature:\\n\", generate_text(char_language_model, \"t\", temperature=0.2), '\\n')\n",
    "\n",
    "print(\"Medium temperature:\\n\", generate_text(char_language_model, \"t\", temperature=1), '\\n')\n",
    "\n",
    "print(\"High temperature:\\n\", generate_text(char_language_model, \"t\", temperature=2), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>How can we make the generated text more convincing?\n",
    "        <ul>\n",
    "            <li>Tweak everything! More layers, more neurons per layer, more epochs, &hellip;\n",
    "            <li>You could make the windows bigger by increasing <code>n_steps</code> but even LSTM and GRUs,\n",
    "                while better than SimpleRNNs, cannot handle very long sequences.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We could change Char-RNN from being a <b>stateless RNN</b> to being a <b>stateful RNN</b>.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Stateless RNNs and Stateful RNNs (Advanced: ignore)</h2>\n",
    "<ul>\n",
    "    <li><b>Stateless RNN:</b> In a training iteration, \n",
    "        <ul>\n",
    "            <li>will be trained on a batch of random chunks of the text;</li>\n",
    "            <li>hidden state starts at all zeros;</li>\n",
    "            <li>processes the input, step by step;</li>\n",
    "            <li>after the last timestep, throws away the hidden state.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li><b>Stateful RNN:</b>\n",
    "        <ul>\n",
    "            <li>preserve the hidden state at the end of the last timestep;</li>\n",
    "            <li>use it as the initial hidden state for the next batch.</li>\n",
    "        </ul>\n",
    "        This way, we can learn longer patterns despite only back-propagating through short\n",
    "        sequences.\n",
    "    </li>\n",
    "    <li>However, we now must arrange our batches quite carefully.\n",
    "        <ul>\n",
    "            <li>Each input sequence in a batch starts where the corresponding sequence in the previous\n",
    "                batch finished.\n",
    "            </li>\n",
    "            <li>In other words, we must remove the overlapping and the shuffling that we used in the\n",
    "                stateless RNN.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Keras comes with a parameter for its recurrent layers, <code>stateful=True</code>.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A Word about Word-Level Language Models</h1>\n",
    "<ul>\n",
    "    <li>The ideas are similar but the network predicts words instead of characters.</li>\n",
    "    <li>But let's be more exact in our description:\n",
    "        <ul>\n",
    "            <li>The output layer of the character-level language model has one neuron per possible <em>character</em>; \n",
    "                see <code>Dense(max_id,...)</code> above. E.g. if there are 39 possible characters, then there\n",
    "                are 39 neurons in this layer. It outputs 39 probabilities.\n",
    "            </li>\n",
    "            <li>The output layer of a word-level language model has one neuron per <em>word</em> in our vocabulary: tens- or \n",
    "                hundreds-of-thousands of neurons; tens- or hundreds-of-thousands probabilities.\n",
    "            </li>\n",
    "            <li>The word-level language model gives us a problem. The softmax activation function must sum\n",
    "                over the outputs of all the neurons. This is OK if there a few dozen (character-level language\n",
    "                model) but not if there are thousands (word-level model).\n",
    "            </li>\n",
    "            <li>One solution that help speed-up training is called sampled softmax.\n",
    "                Without going into the details, in sampled softmax, the loss is estimated from a <em>sample</em>\n",
    "                of the \n",
    "                outputs, instead of all of them.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "<h1>Pretrained Language Models</h1>\n",
    "<ul>\n",
    "    <li>There has been a lot\n",
    "        of work on producing pretrained models that you can use as layers in your own architecture.\n",
    "    </li>\n",
    "    <li>One famous example is Google's BERT: <a href=\"https://github.com/google-research/bert\">https://github.com/google-research/bert</a>\n",
    "        <ul>\n",
    "            <li>Google claim that this has much improved their search engine's ability to answer\n",
    "                questions (as opposed to traditional keyword search): <a href=\"https://www.blog.google/products/search/search-language-understanding-bert/\">https://www.blog.google/products/search/search-language-understanding-bert/</a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "        Another famous example is GPT-3, releaed in 2020. It has 175 billion parameters and produces text that is hard to distinguish from text produced by humans.\n",
    "        <ul>\n",
    "            <li>Consider this, for example: <a href=\"https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\">https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3</a>\n",
    "            </li>\n",
    "            <li>Or try Dungeons &amp; Dragons game: <a href=\"https://play.aidungeon.io/main/landing\">https://play.aidungeon.io/main/landing</a>\n",
    "                (But note the controversy too: some players were typing words that caused the game to generate stories depicting sexual encounters involving children <a href=\"https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/\">https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/</a>)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>These pretrained models use neural network architectures that go beyond what we've seen so far. For\n",
    "        example, they might be bidirectional; they might use one-dimensional convolutional \n",
    "        layers instead of RNNs/LSTMs/GRUs; and they might use the new Transformer architectures.\n",
    "        We'll (briefly) discuss some of these ideas in the next lecture.\n",
    "    </li>\n",
    "</ul>\n",
    "-->\n",
    "<h1>Language Models and their Applications</h1>\n",
    "<ul>\n",
    "    <li>The most successful Language Models use neural network architectures that go beyond what we've seen so far. For\n",
    "        example, they might be bidirectional; and they might use one-dimensional convolutional \n",
    "        layers instead of RNNs/LSTMs/GRUs. Most likely, they use the new Transformer architecture, which we cover\n",
    "  in the next lecture. BERT and GPT, for example, are Language Models that use the Tranformer architecture.\n",
    "    </li>\n",
    "    <li>Google claims that BERT (<a href=\"https://github.com/google-research/bert\">https://github.com/google-research/bert</a>) has much improved their search engine's ability to answer\n",
    "                questions (as opposed to traditional keyword search): <a href=\"https://www.blog.google/products/search/search-language-understanding-bert/\">https://www.blog.google/products/search/search-language-understanding-bert/</a>\n",
    "    </li>\n",
    "    <!--<li><i>Sunspring</i> is a sci-fi movie whose script was generated by an LSTM trained on existing\n",
    "        movie scripts: <a href=\"http://www.thereforefilms.com/sunspring.html\">http://www.thereforefilms.com/sunspring.html</a>\n",
    "    </li>-->\n",
    "    <li>GPT-3, released in 2020, has 175 billion parameters and produces text that is hard to distinguish from text produced by humans.\n",
    "        <ul>\n",
    "            <li>Consider this, for example: <a href=\"https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3\">https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3</a>\n",
    "            </li>\n",
    "            <li>There are also theatre pieces that it has been involve in, e.g.: <a href=\"https://www.theguardian.com/stage/2021/aug/24/rise-of-the-robo-drama-young-vic-creates-new-play-using-artificial-intelligence\">https://www.theguardian.com/stage/2021/aug/24/rise-of-the-robo-drama-young-vic-creates-new-play-using-artificial-intelligence</a></li>\n",
    "            <li>Or try Dungeons &amp; Dragons game: <a href=\"https://play.aidungeon.io/main/landing\">https://play.aidungeon.io/main/landing</a>\n",
    "                (But note the controversy too: some players were typing words that caused the game to generate stories depicting sexual encounters involving children <a href=\"https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/\">https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/</a>)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Meta's Galactica is Language Model trained on 48 million examples of scientific articles. Meta claimed Galactica “can summarize academic papers, solve math problems, generate Wiki articles, write scientific code, annotate molecules and proteins, and more.” Made available on the 15th November 2022, the public demo was taken down on the 17th November after a storm of criticism. Criticism focused on its inability to distinguish truth from fiction &mdash; which is a skill that a tool to assist scientists ought to have! E.g. <a href=\"https://twitter.com/Michael_J_Black/status/1593133722316189696\">https://twitter.com/Michael_J_Black/status/1593133722316189696</a>.\n",
    "    </li>\n",
    "    <li>These ideas are used to create software that can paraphrase, e.g. <a href=\"https://quillbot.com/\">https://quillbot.com/</a>\n",
    "        <!-- https://twitter.com/mattlodder/status/1488894436801232896?t=nK_Of2cKjrQpZa6-yHgtKA&s=03 -->\n",
    "    </li>\n",
    "    <!--\n",
    "    <li>There are researchers who are trying to generate explanations using these techniques.</li>\n",
    "    <li>But let's look briefly at image captioning, machine translation and question-answering.</li>\n",
    "    -->\n",
    "    <li>The same ideas lie behind Microsoft's <a href=\"https://copilot.github.com/\">Copilot</a>\n",
    "        <ul>\n",
    "            <li>Available as a VSCode Extension, it has been trained on billions of lines of public code.</li>\n",
    "            <li>It's like an autocomplete, but for whole lines of code and even entire functions.</li>\n",
    "            <li>Does it make programmers obsolete?\n",
    "                <ul>\n",
    "                    <li>Since it understands nothing, there are claims that it often writes buggy code, e.g.\n",
    "                        <a href=\"https://twitter.com/asmeurer/status/1410399693025153028\">https://twitter.com/asmeurer/status/1410399693025153028</a>\n",
    "                    </li>\n",
    "                    <li>There are concerns about whether what it has been trained on is truly 'public' and\n",
    "                        whether the code it generates is original enough to escape citation/licence problems.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>OpenAI's Codex is similar to Microsoft's Copilot: <a href=\"https://openai.com/blog/openai-codex/\">https://openai.com/blog/openai-codex/</a> but, instead of code completion, you tell it what you want in English. Take a look &mdash; it's really cool.\n",
    "    </li>\n",
    "    <li>We can generate music in this way too, e.g. <a href=\"https://folkrnn.org/\">https://folkrnn.org/</a></li>\n",
    "    <li>The biggest stir most recently was Open AI's <i>Chat-GPT</i> (<a href=\"https://openai.com/blog/chatgpt/\">https://openai.com/blog/chatgpt/</a>). The demo is <a href=\"https://chat.openai.com/auth/login\">here</a> but sometimes unavailable due to demand. It uses GPT for conversation. But, again, the main problems are: it has no understanding; it cannot reason; it cann distinguish fact from fiction. It just makes shit up.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>A Final Thought</h1> \n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/parrot.png\" />\n",
    "            <figcaption>\n",
    "                Image from <a href=\"https://twitter.com/cuducos\">Cuducos</a>\n",
    "            </figcaption>\n",
    "        </figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
