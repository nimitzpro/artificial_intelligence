{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Conclusions\n",
    "</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br>\n",
    "    School of Computer Science and Information Technology<br>\n",
    "    University College Cork\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h1>Initialization</h1>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Introduction</h1>\n",
    "<ul>\n",
    "    <li>In two 5-credit modules, we've only scratched the surface of AI.</li>\n",
    "    <li>But today we'll step back.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Applied AI and Artificial General Intelligence</h1>\n",
    "<ul>\n",
    "    <li>Let's start with a quote from Elon Musk (3rd July 2021):\n",
    "        <figure>\n",
    "            <img src=\"images/musk_quote.jpeg\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Skill-Based, Applied AI</h1>\n",
    "<ul>\n",
    "    <li>Everything that passes for \"AI\" at the moment involves building special-purpose systems capable of\n",
    "        handling narrow, well-described tasks.\n",
    "    </li>\n",
    "    <li>We measure success using performance measures that quantify the skill of the system at the given task.\n",
    "        <ul>\n",
    "            <li>By these measures, we sometimes achieve human-level or supra-human-level performance.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Appropriately deployed, these systems are useful tools.</li>\n",
    "    <li>But these systems are brittle (examples below), which is not what we expect of \"intelligence\".</li>\n",
    "    <li>The main way we are building these systems is supervised learning, so let's focus in on that.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Problems with Supervised Learning</h1>\n",
    "<ul>\n",
    "    <li>Objectives:\n",
    "        <ul>\n",
    "            <li>Learning seeks to minimise a loss function, usually prediction error.</li>\n",
    "            <li>But this function is only a proxy for what we care about in the real world.</li>\n",
    "            <li>E.g. in a recommender system we might minimise star rating prediction error but what we\n",
    "                care about is satisfaction (relevance, surprise, diversity, &hellip;).\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Confidence:\n",
    "        <ul>\n",
    "            <li>We assume training, validation and test sets are representative of the population.</li>\n",
    "            <li>But not many practical systems can also output how confident they are in a prediction.\n",
    "                <ul>\n",
    "                    <li>(The probabilities produced by an output neuron are often not a good measure of \n",
    "                        confidence &mdash; see below.)\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>And, almost no systems can recognize when an unseen example falls outside the distribution on\n",
    "                which the system was trained.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Robustness:\n",
    "        <ul>\n",
    "            <li>How vulnerable is the system to noise? or to deliberate attack?</li>\n",
    "            <li>Neural networks were supposed to be more robust than more brittle technologies such as\n",
    "                Decision Trees.\n",
    "            </li>\n",
    "            <li>But <i>adversarial examples</i>: an example formed by applying small but intentional\n",
    "                perturbations to an original example, such that the adversarial example\n",
    "                results in the model outputting an incorrect answer with high probability but typically\n",
    "                does not change the way humans would label the example\n",
    "                <figure style=\"display: flex\" >\n",
    "                    <img src=\"images/adv1.png\" /> <img src=\"images/adv2.png\" />\n",
    "                </figure>\n",
    "            </li>\n",
    "            <li>Relatedly, Hosseini and Poovendran: <a href=\"https://labs.ece.uw.edu/nsl/papers/hossein_2017.pdf\">Deep Neural Networks Do Not Recognize\n",
    "Negative Images</a></li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Bias:\n",
    "        <ul>\n",
    "            <li>There may be intentional or unintentional bias in the choice of:\n",
    "                <ul>\n",
    "                    <li>the loss function;</li>\n",
    "                    <li>the features and how they are preprocessed; and</li>\n",
    "                    <li>the training examples.</li>\n",
    "                </ul>\n",
    "            </li>  \n",
    "            <li>Some of the bias may be systemic (i.e. societal such as sexism, etc.); some of it may be \n",
    "                selection bias (due to deficiencies in\n",
    "                the data collection process).\n",
    "            </li>\n",
    "            <li>Examples:\n",
    "                <ul>\n",
    "                    <li><a href=\"https://www.jefftk.com/p/detecting-tanks\">https://www.jefftk.com/p/detecting-tanks</a> (this one is an urban myth)</li>\n",
    "                    <li><a href=\"https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html\">https://www.nytimes.com/2018/02/09/technology/facial-recognition-race-artificial-intelligence.html</a></li>\n",
    "                    <li><a href=\"https://www.theguardian.com/technology/2015/jul/01/google-sorry-racist-auto-tag-photo-app\">https://www.theguardian.com/technology/2015/jul/01/google-sorry-racist-auto-tag-photo-app</a></li>\n",
    "                    <li><a href=\"https://www.theguardian.com/us-news/2018/jan/17/software-no-more-accurate-than-untrained-humans-at-judging-reoffending-risk\">https://www.theguardian.com/us-news/2018/jan/17/software-no-more-accurate-than-untrained-humans-at-judging-reoffending-risk</a></li>\n",
    "                    <li>From MIT Review: <a href=\"https://www.technologyreview.com/2021/07/30/1030329/machine-learning-ai-failed-covid-hospital-diagnosis-pandemic/\">Hundreds of AI tools have been built to catch covid. None of them helped.</a></li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Trust (is this system safe?):\n",
    "        <ul>\n",
    "            <li>We've implied that once you find a system whose test error is low, you deploy it.</li>\n",
    "            <li>In reality, decisions to deploy are not, or should not be, so straightforward.\n",
    "                <ul>\n",
    "                    <li>Think about the apprenticeship served by medical students and junior doctors.</li>\n",
    "                    <li>Think about all the ways that Google and others are testing their self-driving vehicles\n",
    "                        <ul>\n",
    "                            <li>E.g. <a href=\"https://cacm.acm.org/magazines/2018/2/224621-a-comprehensive-self-driving-car-test/fulltext?mobile=false\">https://cacm.acm.org/magazines/2018/2/224621-a-comprehensive-self-driving-car-test/fulltext</a></li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Distribution shifts: \n",
    "        <ul>\n",
    "            <li>For error (or accuracy) estimation to make sense, we assumed that our training and test sets were drawn from the same distribution.</li>\n",
    "            <li>Suppose our model performs well enough on the test set that we decide to deploy it. We will be asked to make predictions for unseen examples. What is there to say that these will aso be drawn from the same distribution?</li>\n",
    "            <li>There are several types of distribution shift that may mean that our model performs worse in practice than it did during its development:\n",
    "                <ul>\n",
    "                    <li>Covariate shift: Covariate is fancy word for feature. So covariate shift is a shift in the distribution of feature values. For example, maybe we developed a classifier using well-centered front-on close-ups of faces but we try to use our classifier on images taken during daily use.</li>\n",
    "                    <li>Label shift: This is a shift in the distribution of the targets. For example, maybe a certain disease becomes more prevalent.</li>\n",
    "                    <li>Concept shift: In some domains, classes themsleves undergo redefinition. For example, classifiers that say whether an item of clothing is fashionable or not; or a classifier that says whether a joke is funny or not; or a classifier that says whether a tweet is offensive or not.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>To what extent, do interpretable models and explanations solve the problems listed above?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Are we even doing AI?</h1>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td rowspan=\"4\" style=\"border: 1px solid black;\"><img src=\"images/chollet.jpg\" /></td>\n",
    "        <td style=\"border: 1px solid black;\">\"...the problem of cognition has almost no overlap with supervised learning, \n",
    "             what we're currently good at -- learning to map space x to space y given a dense \n",
    "             sampling of x-cross-y\"\n",
    "             <a href=\"https://twitter.com/fchollet/status/947119817286438913?s=03\">https://twitter.com/fchollet/status/947119817286438913?s=03</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black;\">\"... parametric models trained with gradient descent make it easy to automate something, but have little ability to deviate from the patterns they've learned. Meanwhile, the real world is full of surprises, and handling it requires the ability to adapt.\"\n",
    "             <a href=\"https://twitter.com/fchollet/status/1373116543626735624\">https://twitter.com/fchollet/status/1373116543626735624</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black;\">\"Arguably, the main challenges of cognition are not even represented in classic supervised learning \n",
    "             from big data -- exploration, goal-setting, open-endedness, abstraction, extreme generalization, \n",
    "             learning from few data points...\"\n",
    "             <a href=\"https://twitter.com/fchollet/status/947439738008547328?s=03\">https://twitter.com/fchollet/status/947439738008547328?s=03</a>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid black;\">\"For all the progress made, it seems like almost all important questions in AI remain unanswered. \n",
    "             Many have not even been properly asked yet.\"\n",
    "             <a href=\"https://twitter.com/fchollet/status/837188765500071937\">https://twitter.com/fchollet/status/837188765500071937</a>\n",
    "         </td>\n",
    "     </tr>\n",
    "</table>\n",
    "<p>\n",
    "    Even when it comes to learning, humans generalise quickly from few (probably unlabeled) examples\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Artificial General Intelligence (AGI)</h1>\n",
    "<ul>\n",
    "    <li>AGI has many definitions &mdash; some people aren't even convinced it's a coherent concept.</li>\n",
    "    <li>One definition (although I doubt he'd want to use the phrase AGI) is from Chollet:\n",
    "        <quote>\n",
    "            The intelligence of a system is a measure of its skill-acquisition efficiency over a \n",
    "            scope of tasks, with respect to priors, experience, and generalization difficulty.\n",
    "        </quote>\n",
    "    </li>\n",
    "    <li>In <a href=\"https://arxiv.org/pdf/1911.01547.pdf\">https://arxiv.org/pdf/1911.01547.pdf</a>,\n",
    "        Chollet has proposed a task + dataset (now hosted as a <a href=\"https://www.kaggle.com/c/abstraction-and-reasoning-challenge/overview\">Kaggle competition</a>)\n",
    "        <ul>\n",
    "            <li>work out what relates some pairs of images; use this to complete a final pair;</li>\n",
    "            <li>but the relationships require reasoning, abstraction and analogy.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Is anyone even working on AGI?\n",
    "        <ul>\n",
    "            <li>See the end of the second RL lecture, where we discussed\n",
    "                DeepMind's work training an agent to play mutliple games. Here are the links again:\n",
    "                <a href=\"https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play\">Blog post (with link to videos)</a>; <a href=\"https://deepmind.com/research/publications/open-ended-learning-leads-to-generally-capable-agents\">Longer manuscript</a>.\n",
    "            </li>\n",
    "            <li>Also see DeepMind's Gato system (<a href=\"https://www.deepmind.com/publications/a-generalist-agent\">Blog post</a>). They traing something like a language model on mutliple datasets for different tasks. They claim: \"The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens.\"\n",
    "             </li>\n",
    "             <li>And see Yann LeCun's speculation about the kind of neural network(s) needed to achieve some kind of\n",
    "                 AGI: <a href=\"https://openreview.net/forum?id=BZ5a1r-kVsf\">paper</a>.\n",
    "             </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Common sense &mdash; the least common of all the senses</h1>\n",
    "<ul>\n",
    "    <li>As well as reasoning, abstraction and analogy, current AI systems lack common sense.</li>\n",
    "    <li>What is common sense knowledge?\n",
    "        <ul>\n",
    "            <li>E.g. Marvin Minsky asks: Can you push a car with a feather?</li>\n",
    "            <li>E.g. look at these photos and answer some questions: when, where, who, why, what happened\n",
    "                just before, what will happen after, &hellip;\n",
    "                <figure style=\"display: flex; align-items: flex-start\">\n",
    "                    <img src=\"images/grad.jpeg\" /> <img src=\"images/coffee.jpeg\" /> \n",
    "                </figure>\n",
    "            </li>\n",
    "            <li>E.g. Winograd sentences (which were the subject of a <a href=\"http://commonsensereasoning.org/winograd.html\">competition</a>):\n",
    "                <ul>\n",
    "                    <li>\"The town councilors refused to give the demonstrators a permit because they feared violence.\"<br>\n",
    "                        Who feared violence? The town councilors or the demonstrators?\n",
    "                    </li>\n",
    "                    <li>\"The town councilors refused to give the demonstrators a permit because they advocated violence.\"<br>\n",
    "                        Who advocated violence?\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>E.g. study these two stories (from Searle, John. R. (1980) Minds, brains, and programs. \n",
    "                Behavioral and Brain Sciences 3 (3): 417-457) and tell me whether the man ate the hamburger:\n",
    "                <ul>\n",
    "                    <li>\"A man went into a restaurant and ordered a hamburger. When the hamburger arrived it \n",
    "                        was burned to a crisp, and the man stormed out of the restaurant angrily, without paying \n",
    "                        for the hamburger or leaving a tip.\"\n",
    "                    </li>\n",
    "                    <li>\"A man went into a restaurant and ordered a hamburger; when the hamburger came he was \n",
    "                        very pleased with it; and as he left the restaurant he gave the waitress a large tip\n",
    "                        before paying his bill.\"\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Intuitive physics; intuitive biology; intuitive psychology; etc:\n",
    "                <ul>\n",
    "                    <li>Our everyday theories of physics (objects, forces, &hellip;), biology (e.g. taxonomies),\n",
    "                        psychology (beliefs, desires, intentions, sensations, emotions, &hellip;), society \n",
    "                        (posessions, crime, marriage, &hellip;) &hellip;\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We use this knowledge to constrain what we learn and what we predict.</li>\n",
    "    <li>Can computers acquire this knowledge? How? What are the consequences of being deficient in certain\n",
    "    parts of this knowledge?\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Finally, let's revisit some material that was in lecture 1 of CS4618 AI 1\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>Is AI even possible?</h1>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"border: 1px solid blue; vertical-align: top; text-align: left;\">\n",
    "            <b>No</b>: there's a special and essential ingredient that can't be replicated, e.g. soul, spirit,\n",
    "            consciousness, free will, creativity, humour, &hellip;\n",
    "            <p>\n",
    "            Perhaps we can <b>simulate</b> intelligence:\n",
    "            </p>\n",
    "            <ul>\n",
    "                <li>Outwardly, systems may <em>behave as if</em> intelligent.</li>\n",
    "                <li>But the way they achieve this behaviour (the internal process) doesn't qualify as true\n",
    "                    thinking.\n",
    "                </li>\n",
    "            </ul>\n",
    "        </td>\n",
    "        <td style=\"border: 1px solid blue; vertical-align: top;\">\n",
    "            <b>Yes</b>, we can build <b>true human-like</b> intelligence.\n",
    "        </td>\n",
    "        <td style=\"border: 1px solid blue; vertical-align: top;\">\n",
    "            <b>Yes</b>, we can build true intelligences but they won't necessarily be like us.<br />\n",
    "            AI = <b>alien intelligence</b>.\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<ul>\n",
    "    <li>Where do you sit in this table? Or, do you have a different view?</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>What are the risks?</h1>\n",
    "<table style=\"width: 100%;\">\n",
    "                    <tr>\n",
    "                        <td style=\"border-right-width: 0\">\n",
    "                            <img style=\"width: 100px\" src=\"images/musk.jpg\" />\n",
    "                        </td>\n",
    "                        <td style=\"border-left-width: 0; border-right: 1px solid black;\">\n",
    "                            \"&hellip;the most serious threat to the survival of the human race&hellip;\"\n",
    "                        </td>\n",
    "                        <td style=\"border-right-width: 0\">\n",
    "                            <img style=\"width: 100px\" src=\"images/hawking.jpg\" />\n",
    "                        </td>\n",
    "                        <td style=\"border-left-width: 0\">\n",
    "                            \"The development of full artificial intelligence could spell the end of the human\n",
    "                            race&hellip;It would take off on its own, and re-design itself at an ever \n",
    "                            increasing rate.\"\n",
    "                        </td>\n",
    "                    </tr>\n",
    "                </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1>So what are the real risks?</h1>\n",
    "<ul>\n",
    "    <li>We can analyse the dangers in terms of:\n",
    "        <ul>\n",
    "            <li>malevolent goals, and</li>\n",
    "            <li>destructive methods for achieving benevolent or malevolent goals (e.g. methods that have\n",
    "                unacceptable externalities)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In the near to medium term, we should worry much less about super-intelligences that develop their own\n",
    "        malevolent goals (e.g. to kill, enslave or displace us)\n",
    "    </li>\n",
    "    <li>Rather, we should worry about governments, corporations and individuals intentionally or \n",
    "        unintentionally building AI systems that try to achieve their goals using destructive methods<br />\n",
    "        E.g.\n",
    "        <ul>\n",
    "            <li>so-called 'collateral damage' from autonomous weapons</li>\n",
    "            <li>displacement of employment</li>\n",
    "            <li>reduction in the economic, military or social value of some classes of human beings</li>\n",
    "            <li>invasions of privacy</li>\n",
    "            <li>'filter bubbles' or 'echo chambers'</li>\n",
    "            <li>adoption or perpetuation of bias and prejudice</li>\n",
    "            <li>data-intensive AI restricted to a handful of hardware-rich and data-rich corporations</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Stuart Russell, a major figure in the field, suggests we need to approach AI with a different mindset for dealing with both the shorter-term and longer-term risks. Not this:\n",
    "        <ul>\n",
    "            <li>Machines are intelligent to the extent that their actions can be expected to meet their objectives.</li>\n",
    "        </ul>\n",
    "       But this:\n",
    "        <ul>\n",
    "            <li>Machines are beneficial to the extent that their actions can be expected to meet our objectives.</li>\n",
    "        </ul>\n",
    "    </li>   \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Where Next?</h1>\n",
    "<ul>\n",
    "    <li>Students often ask what to look at next, after these modules. Obviously, the answer depends on what areas interest you. I can perhaps give some recommendations if you ask me. But here I just list some books/web sites that I have found to be useful. The first two have influenced the CS4618 &amp; CS4619 modules a fair bit. But the others are good too.</li>\n",
    "    <li>François Chollet: <i>Deep Learning with Python (2nd edn)</i>, Manning Publications, 2021. Publisher's web site: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">https://www.manning.com/books/deep-learning-with-python-second-edition</a>; code: <a href=\"https://github.com/fchollet/deep-learning-with-python-notebooks\">https://github.com/fchollet/deep-learning-with-python-notebooks</a></li>\n",
    "    <li>Aurélien Géron: <i>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd edn)</i>, O' Reilly, 2019. Publisher's web site: <a href=\"https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\">https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\"</a>; code: <a href=\"https://github.com/ageron/handson-ml2\">https://github.com/ageron/handson-ml2</a>. Note: 3rd edition may be available by now.</li>\n",
    "    <li>Seebastian Raschka , Yuxi (Hayden) Liu , Vahid Mirjalili: <i>Machine Learning with PyTorch and Scikit-Learn</i>, Packt Publications, 2022. Publisher's web site: <a href=\"https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312\">https://www.packtpub.com/product/machine-learning-with-pytorch-and-scikit-learn/9781801819312</a>; code: <a href=\"https://github.com/rasbt/machine-learning-book\">https://github.com/rasbt/machine-learning-book</a></li>\n",
    "    <li>Aston Zhang, Zachary C. Lipton, Mu Li, Alexander J. Smola: <i>Dive into Deep Learning</i>. Interactive book: <a href=\"http://d2l.ai/\">http://d2l.ai/</a></li>\n",
    "    <li>Jeff Heaton: <i>Applications of Deep Neural Networks with Keras</i>. PDF available: <a href=\"https://www.heatonresearch.com/book/applications-deep-neural-networks-keras.html\">https://www.heatonresearch.com/book/applications-deep-neural-networks-keras.html</a>; code: <a href=\"https://github.com/jeffheaton/t81_558_deep_learning\">https://github.com/jeffheaton/t81_558_deep_learning</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
